<script type="text/javascript">
        var gk_isXlsx = false;
        var gk_xlsxFileLookup = {};
        var gk_fileData = {};
        function filledCell(cell) {
          return cell !== '' && cell != null;
        }
        function loadFileData(filename) {
        if (gk_isXlsx && gk_xlsxFileLookup[filename]) {
            try {
                var workbook = XLSX.read(gk_fileData[filename], { type: 'base64' });
                var firstSheetName = workbook.SheetNames[0];
                var worksheet = workbook.Sheets[firstSheetName];

                // Convert sheet to JSON to filter blank rows
                var jsonData = XLSX.utils.sheet_to_json(worksheet, { header: 1, blankrows: false, defval: '' });
                // Filter out blank rows (rows where all cells are empty, null, or undefined)
                var filteredData = jsonData.filter(row => row.some(filledCell));

                // Heuristic to find the header row by ignoring rows with fewer filled cells than the next row
                var headerRowIndex = filteredData.findIndex((row, index) =>
                  row.filter(filledCell).length >= filteredData[index + 1]?.filter(filledCell).length
                );
                // Fallback
                if (headerRowIndex === -1 || headerRowIndex > 25) {
                  headerRowIndex = 0;
                }

                // Convert filtered JSON back to CSV
                var csv = XLSX.utils.aoa_to_sheet(filteredData.slice(headerRowIndex)); // Create a new sheet from filtered array of arrays
                csv = XLSX.utils.sheet_to_csv(csv, { header: 1 });
                return csv;
            } catch (e) {
                console.error(e);
                return "";
            }
        }
        return gk_fileData[filename] || "";
        }
        </script><!DOCTYPE html>
<html>
<head>
    <title>Modelo ARMA - Tesla, NVIDIA, Meta</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        pre { background: #f4f4f4; padding: 15px; border-radius: 5px; }
        .code { font-family: 'Courier New', Courier, monospace; font-size: 14px; }
    </style>
</head>
<body>
    <h1>Modelo ARMA - Tesla, NVIDIA, Meta</h1>
    <p>Python script for ARIMA modeling, unit root tests, cointegration tests, and forecasting.</p>
    <pre class="code"><code>
# -*- coding: utf-8 -*-
"""ModeloARMA25Abril.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zDyfMvJrbllwNnSNgFD9QthQvMtk0Hhs
"""

# Import required libraries
import pandas as pd
import numpy as np
from google.colab import files
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.vector_ar.vecm import coint_johansen
import matplotlib.pyplot as plt

# Upload single Excel file
print("Please upload Excel file containing Tesla, NVIDIA, and Meta data in separate sheets")
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# Read Excel file with multiple sheets
excel_file = pd.ExcelFile(filename)

# Print available sheet names
print("\nAvailable sheets in the Excel file:", excel_file.sheet_names)

# Ask user for sheet names
tesla_sheet = input("Enter the sheet name for Tesla data: ")
nvidia_sheet = input("Enter the sheet name for NVIDIA data: ")
meta_sheet = input("Enter the sheet name for Meta data: ")

# Read data from specified sheets
try:
    tesla_df = pd.read_excel(filename, sheet_name=tesla_sheet)
    nvidia_df = pd.read_excel(filename, sheet_name=nvidia_sheet)
    meta_df = pd.read_excel(filename, sheet_name=meta_sheet)
except ValueError as e:
    print(f"Error: One or more sheet names not found. Available sheets: {excel_file.sheet_names}")
    raise

# Print available columns for each DataFrame
print("\nTesla DataFrame columns:", list(tesla_df.columns))
print("NVIDIA DataFrame columns:", list(nvidia_df.columns))
print("Meta DataFrame columns:", list(meta_df.columns))

# Ask user for the correct column name
close_column = input("Please enter the column name containing closing prices: ")

# Extract closing prices using user-specified column name
try:
    tesla_close = tesla_df[close_column]
    nvidia_close = nvidia_df[close_column]
    meta_close = meta_df[close_column]
except KeyError:
    print(f"Error: Column '{close_column}' not found in one or more DataFrames")
    print("Please check the column names and try again")
    raise

# Ensure all series have the same length
min_length = min(len(tesla_close), len(nvidia_close), len(meta_close))
tesla_close = tesla_close[:min_length]
nvidia_close = nvidia_close[:min_length]
meta_close = meta_close[:min_length]

# Function for unit root tests with interpretation
def unit_root_tests(series, name):
    print(f"\nUnit Root Tests for {name}:")

    # ADF Test
    adf_result = adfuller(series)
    print("ADF Test:")
    print(f'ADF Statistic: {adf_result[0]:.4f}')
    print(f'p-value: {adf_result[1]:.4f}')
    print(f'Critical Values: {adf_result[4]}')
    print("Interpretation:")
    if adf_result[1] < 0.05:
        print(f"  - p-value < 0.05: Reject null hypothesis - {name} is stationary")
    else:
        print(f"  - p-value >= 0.05: Fail to reject null - {name} may be non-stationary")

    # KPSS Test
    kpss_result = kpss(series)
    print("\nKPSS Test:")
    print(f'KPSS Statistic: {kpss_result[0]:.4f}')
    print(f'p-value: {kpss_result[1]:.4f}')
    print(f'Critical Values: {kpss_result[3]}')
    print("Interpretation:")
    if kpss_result[1] < 0.05:
        print(f"  - p-value < 0.05: Reject null hypothesis - {name} is non-stationary")
    else:
        print(f"  - p-value >= 0.05: Fail to reject null - {name} may be stationary")

# Perform unit root tests
unit_root_tests(tesla_close, "Tesla")
unit_root_tests(nvidia_close, "NVIDIA")
unit_root_tests(meta_close, "Meta")

# Cointegration test with interpretation
def cointegration_test(df):
    result = coint_johansen(df, det_order=0, k_ar_diff=1)
    print("\nJohansen Cointegration Test for Tesla, NVIDIA, and Meta:")
    print(f"Trace statistic: {result.lr1}")
    print(f"Critical values (90%, 95%, 99%): {result.cvt}")
    print("Interpretation:")
    for i in range(len(result.lr1)):
        if result.lr1[i] > result.cvt[i, 1]:
            print(f"  - r = {i}: Cointegration exists at 95% confidence level")
            print(f"    Trace statistic ({result.lr1[i]:.2f}) > 95% critical value ({result.cvt[i, 1]:.2f})")
        else:
            print(f"  - r = {i}: No cointegration at 95% confidence level")
            print(f"    Trace statistic ({result.lr1[i]:.2f}) <= 95% critical value ({result.cvt[i, 1]:.2f})")
    if result.lr1[0] > result.cvt[0, 1]:
        print("Conclusion: Tesla, NVIDIA, and Meta are cointegrated - they share a long-run equilibrium relationship")
    else:
        print("Conclusion: No evidence of cointegration among Tesla, NVIDIA, and Meta")

# Prepare data for cointegration
coint_df = pd.DataFrame({
    'Tesla': tesla_close,
    'NVIDIA': nvidia_close,
    'Meta': meta_close
}).dropna()
cointegration_test(coint_df)

# Function to find best ARIMA model with interpretation
def find_best_arima(series, name, max_p=3, max_d=2, max_q=3):
    best_aic = float('inf')
    best_order = None

    for p in range(max_p + 1):
        for d in range(max_d + 1):
            for q in range(max_q + 1):
                try:
                    model = ARIMA(series, order=(p, d, q))
                    results = model.fit()
                    if results.aic < best_aic:
                        best_aic = results.aic
                        best_order = (p, d, q)
                except:
                    continue

    print(f"\nBest ARIMA model for {name}:")
    print(f"Order: {best_order}")
    print(f"AIC: {best_aic:.2f}")
    print("Interpretation:")
    print(f"  - p={best_order[0]}: {best_order[0]} autoregressive term(s)")
    print(f"  - d={best_order[1]}: {best_order[1]} difference(s) needed for stationarity")
    print(f"  - q={best_order[2]}: {best_order[2]} moving average term(s)")
    return best_order

# Find and fit best ARIMA models
tesla_order = find_best_arima(tesla_close, "Tesla")
nvidia_order = find_best_arima(nvidia_close, "NVIDIA")
meta_order = find_best_arima(meta_close, "Meta")

# Fit final ARIMA models
tesla_model = ARIMA(tesla_close, order=tesla_order).fit()
nvidia_model = ARIMA(nvidia_close, order=nvidia_order).fit()
meta_model = ARIMA(meta_close, order=meta_order).fit()

# Forecast next 30 periods
forecast_steps = 30
tesla_forecast = tesla_model.forecast(steps=forecast_steps)
nvidia_forecast = nvidia_model.forecast(steps=forecast_steps)
meta_forecast = meta_model.forecast(steps=forecast_steps)

# Create forecast index
last_index = len(tesla_close) - 1
forecast_index = range(last_index + 1, last_index + 1 + forecast_steps)

# Plot original series with forecasts
plt.figure(figsize=(12,6))
plt.plot(tesla_close, label='Tesla Historical')
plt.plot(forecast_index, tesla_forecast, label='Tesla Forecast', color='red')
plt.plot(nvidia_close, label='NVIDIA Historical')
plt.plot(forecast_index, nvidia_forecast, label='NVIDIA Forecast', color='green')
plt.plot(meta_close, label='Meta Historical')
plt.plot(forecast_index, meta_forecast, label='Meta Forecast', color='blue')
plt.title('Tesla, NVIDIA, and Meta Closing Prices with Forecasts')
plt.legend()
plt.show()

# Detailed forecast plot with confidence intervals and interpretation
def plot_forecast(model, series, name, steps=30):
    forecast_obj = model.get_forecast(steps=steps)
    forecast = forecast_obj.predicted_mean
    conf_int = forecast_obj.conf_int()

    forecast_index = range(len(series), len(series) + steps)

    plt.figure(figsize=(12,6))
    plt.plot(series, label=f'{name} Historical')
    plt.plot(forecast_index, forecast, label='Forecast', color='red')
    plt.fill_between(forecast_index,
                    conf_int.iloc[:, 0],
                    conf_int.iloc[:, 1],
                    color='pink',
                    alpha=0.3,
                    label='95% Confidence Interval')
    plt.title(f'{name} Price Forecast')
    plt.legend()
    plt.show()

    # Forecast interpretation
    last_value = series.iloc[-1]
    mean_forecast = forecast.mean()
    print(f"\nForecast Interpretation for {name}:")
    print(f"Last observed value: {last_value:.2f}")
    print(f"Average forecast value: {mean_forecast:.2f}")
    print(f"Forecast change: {mean_forecast - last_value:.2f}")
    if mean_forecast > last_value:
        print("Trend: Upward forecast trend")
    elif mean_forecast < last_value:
        print("Trend: Downward forecast trend")
    else:
        print("Trend: Flat forecast trend")
    print(f"95% CI range at period {steps}: [{conf_int.iloc[-1, 0]:.2f}, {conf_int.iloc[-1, 1]:.2f}]")

# Generate detailed forecast plots and interpretations
plot_forecast(tesla_model, tesla_close, "Tesla")
plot_forecast(nvidia_model, nvidia_close, "NVIDIA")
plot_forecast(meta_model, meta_close, "Meta")

# Print forecast values
print("\nTesla Forecast Values (next 5 periods):")
print(tesla_forecast[:5])
print("\nNVIDIA Forecast Values (next 5 periods):")
print(nvidia_forecast[:5])
print("\nMeta Forecast Values (next 5 periods):")
print(meta_forecast[:5])
    </code></pre>
</body>
</html>